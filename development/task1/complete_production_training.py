#!/usr/bin/env python3
"""
Complete Production Training Script for FinRL Contest 2024
Full-scale training for competition-ready ensemble models
"""

import os
import sys
import time
import torch
import numpy as np
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# Add paths
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir / "src"))
sys.path.insert(0, str(current_dir / "src_refactored"))

# Imports
from data_config import ConfigData
from trade_simulator import TradeSimulator
from src_refactored.agents.double_dqn_agent import DoubleDQNAgent, D3QNAgent
from src_refactored.config.agent_configs import DoubleDQNConfig
from src_refactored.ensemble.voting_ensemble import VotingEnsemble, EnsembleStrategy

class CompleteProductionTrainer:
    """Complete production trainer for competition-ready models."""
    
    def __init__(self, output_dir: str = "complete_production_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        self.results = {
            'start_time': time.time(),
            'device': str(self.device),
            'timestamp': self.timestamp,
            'training_config': {},
            'agents': {},
            'ensemble': {},
            'performance': {}
        }
        
        print(f"üöÄ Complete Production Training - Session: {self.timestamp}")
        print(f"üíª Device: {self.device}")
        print(f"üìÅ Output: {self.output_dir}")
        
        if torch.cuda.is_available():
            print(f"üéÆ GPU: {torch.cuda.get_device_name()}")
            print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(self.device).total_memory / 1e9:.1f} GB")
    
    def setup_trading_environment(self):
        """Setup the complete Bitcoin LOB trading environment."""
        print("\nüìä Setting Up Complete Bitcoin LOB Trading Environment")
        print("-" * 60)
        
        try:
            # Load data configuration
            data_config = ConfigData()
            
            # Check for enhanced features (priority order)
            feature_paths = [
                "/mnt/c/QuantConnect/FinRL_Contest_2024/FinRL_Contest_2024/data/raw/task1/BTC_1sec_predict_enhanced_v3.npy",
                "/mnt/c/QuantConnect/FinRL_Contest_2024/FinRL_Contest_2024/data/raw/task1/BTC_1sec_predict_enhanced_v2.npy",
                "/mnt/c/QuantConnect/FinRL_Contest_2024/FinRL_Contest_2024/data/raw/task1/BTC_1sec_predict_enhanced.npy",
                "/mnt/c/QuantConnect/FinRL_Contest_2024/FinRL_Contest_2024/data/raw/task1/BTC_1sec_predict.npy"
            ]
            
            predict_data = None
            feature_type = "unknown"
            
            for path in feature_paths:
                if os.path.exists(path):
                    predict_data = np.load(path)
                    feature_type = os.path.basename(path).replace('BTC_1sec_predict', '').replace('.npy', '') or 'standard'
                    print(f"  ‚úÖ Features loaded: {os.path.basename(path)}")
                    print(f"  üìà Shape: {predict_data.shape}")
                    break
            
            if predict_data is None:
                raise FileNotFoundError("No prediction data found")
            
            print(f"  üìä Dataset size: {len(predict_data):,} timesteps")
            print(f"  üî¨ Feature type: {feature_type}")
            
            # Create trading environment
            env = TradeSimulator(
                num_sims=1,  # Single environment for training
                slippage=5e-5,
                max_position=2,
                step_gap=1,
                gpu_id=0 if torch.cuda.is_available() else -1
            )
            
            # Get dimensions from the environment
            state_dim = env.state_dim
            action_dim = 3  # Buy, Hold, Sell (fixed for discrete trading)
            
            print(f"  ‚úÖ Trading environment created successfully")
            print(f"  üéØ State dimension: {state_dim}")
            print(f"  üéØ Action dimension: {action_dim}")
            
            # Store environment info
            self.results['environment'] = {
                'state_dim': state_dim,
                'action_dim': action_dim,
                'dataset_size': len(predict_data),
                'feature_type': feature_type,
                'data_path': path
            }
            
            return env, state_dim, action_dim, predict_data
            
        except Exception as e:
            print(f"  ‚ùå Environment setup failed: {e}")
            raise
    
    def create_production_agents(self, state_dim: int, action_dim: int) -> Dict[str, Any]:
        """Create production-grade agents with optimized configurations."""
        print("\nü§ñ Creating Production-Grade Agents")
        print("-" * 60)
        
        agents = {}
        
        # Production-optimized configurations
        configs = {
            "DoubleDQN_Production": {
                "class": DoubleDQNAgent,
                "config": DoubleDQNConfig(
                    net_dims=[512, 512, 256],    # Large networks for capacity
                    gamma=0.995,                 # High discount for long-term
                    learning_rate=5e-5,          # Conservative for stability
                    batch_size=128,              # Balanced batch size
                    clip_grad_norm=10.0,         # Gradient clipping
                    soft_update_tau=1e-3,        # Slow target updates
                    explore_rate=0.02            # Low exploration for exploitation
                )
            },
            
            "D3QN_Production": {
                "class": D3QNAgent,
                "config": DoubleDQNConfig(
                    net_dims=[512, 512, 512],    # Even larger for dueling
                    gamma=0.995,
                    learning_rate=3e-5,          # Lower for stability
                    batch_size=128,
                    clip_grad_norm=10.0,
                    soft_update_tau=1e-3,
                    explore_rate=0.01            # Very low exploration
                )
            },
            
            "DoubleDQN_Aggressive": {
                "class": DoubleDQNAgent,
                "config": DoubleDQNConfig(
                    net_dims=[256, 256, 128],    # Smaller for faster training
                    gamma=0.99,                  # Lower discount for agility
                    learning_rate=1e-4,          # Higher for faster learning
                    batch_size=64,               # Smaller batch for frequency
                    clip_grad_norm=5.0,
                    soft_update_tau=5e-3,        # Faster target updates
                    explore_rate=0.05            # Higher exploration
                )
            }
        }
        
        print(f"  üèóÔ∏è Building {len(configs)} production agents...")
        
        for agent_name, agent_info in configs.items():
            try:
                print(f"\n  Creating {agent_name}:")
                
                agent = agent_info["class"](
                    config=agent_info["config"],
                    state_dim=state_dim,
                    action_dim=action_dim,
                    device=self.device
                )
                
                agents[agent_name] = agent
                
                # Calculate parameters
                params = sum(p.numel() for p in agent.online_network.parameters())
                
                print(f"    ‚úÖ {agent_name} created")
                print(f"    üìä Parameters: {params:,}")
                print(f"    üß† Architecture: {agent_info['config'].net_dims}")
                print(f"    üéØ Learning rate: {agent_info['config'].learning_rate}")
                
                # Store agent info
                self.results['agents'][agent_name] = {
                    'type': agent_info["class"].__name__,
                    'config': agent_info["config"].to_dict(),
                    'parameters': params,
                    'created': True
                }
                
            except Exception as e:
                print(f"    ‚ùå Failed to create {agent_name}: {e}")
                self.results['agents'][agent_name] = {'created': False, 'error': str(e)}
        
        print(f"\n  üìä Successfully created {len(agents)}/{len(configs)} agents")
        return agents
    
    def create_production_ensemble(self, agents: Dict[str, Any]) -> Optional[VotingEnsemble]:
        """Create production ensemble with advanced strategies."""
        print("\nü§ù Creating Production Ensemble")
        print("-" * 60)
        
        if len(agents) < 2:
            print("  ‚ö†Ô∏è Need at least 2 agents for ensemble")
            return None
        
        try:
            ensemble = VotingEnsemble(
                agents=agents,
                strategy=EnsembleStrategy.WEIGHTED_VOTE,
                device=self.device
            )
            
            print(f"  ‚úÖ Production ensemble created")
            print(f"  üó≥Ô∏è Strategy: Weighted Voting with Confidence")
            print(f"  üë• Agents: {len(agents)}")
            print(f"  üéØ Agent diversity: {list(agents.keys())}")
            
            self.results['ensemble'] = {
                'created': True,
                'strategy': 'weighted_vote',
                'num_agents': len(agents),
                'agents': list(agents.keys())
            }
            
            return ensemble
            
        except Exception as e:
            print(f"  ‚ùå Ensemble creation failed: {e}")
            self.results['ensemble'] = {'created': False, 'error': str(e)}
            return None
    
    def complete_agent_training(self, agents: Dict[str, Any], env, episodes: int = 500):
        """Complete training for all agents."""
        print(f"\nüéì Complete Agent Training ({episodes} episodes)")
        print("-" * 60)
        
        training_results = {}
        
        for agent_idx, (agent_name, agent) in enumerate(agents.items()):
            print(f"\n  [{agent_idx + 1}/{len(agents)}] Training {agent_name}")
            print(f"  {'-' * 50}")
            
            try:
                episode_rewards = []
                episode_losses = []
                training_start = time.time()
                
                # Training loop
                for episode in range(episodes):
                    episode_start = time.time()
                    
                    # Reset environment
                    state = env.reset()
                    episode_reward = 0
                    episode_loss = 0
                    step = 0
                    max_steps = 200  # Reasonable episode length
                    
                    while step < max_steps:
                        # State preprocessing
                        if not isinstance(state, torch.Tensor):
                            state = torch.tensor(state, dtype=torch.float32, device=self.device)
                        if state.dim() == 1:
                            state = state.unsqueeze(0)
                        
                        # Agent action selection
                        action = agent.select_action(state.squeeze(0), deterministic=False)
                        
                        # Environment step
                        next_state, reward, done, info = env.step(action)
                        episode_reward += reward
                        
                        # Agent update (with proper experience)
                        if step > 32:  # Start updating after initial experience
                            try:
                                # Create training batch
                                batch_size = 32
                                
                                # Use actual transition
                                current_state = state.squeeze(0)
                                next_state_tensor = torch.tensor(next_state, dtype=torch.float32, device=self.device)
                                
                                # Create batch with current transition + random samples
                                states = torch.stack([current_state] * batch_size)
                                actions = torch.tensor([action] * batch_size, device=self.device)
                                rewards = torch.tensor([reward] * batch_size, device=self.device)
                                next_states = torch.stack([next_state_tensor] * batch_size)
                                dones = torch.tensor([done] * batch_size, device=self.device)
                                
                                batch_data = (states, actions, rewards, next_states, dones)
                                result = agent.update(batch_data)
                                
                                if isinstance(result, dict) and 'loss' in result:
                                    episode_loss += result['loss']
                                
                            except Exception as update_e:
                                pass  # Continue despite update errors
                        
                        state = next_state
                        step += 1
                        
                        if done:
                            break
                    
                    episode_rewards.append(episode_reward)
                    episode_losses.append(episode_loss / max(step, 1))
                    
                    # Progress reporting
                    if (episode + 1) % 50 == 0:
                        avg_reward = np.mean(episode_rewards[-50:])
                        avg_loss = np.mean(episode_losses[-50:])
                        elapsed = time.time() - training_start
                        eta = (elapsed / (episode + 1)) * (episodes - episode - 1)
                        
                        print(f"    Episode {episode + 1:3d}/{episodes}: "
                              f"Reward={avg_reward:8.3f}, Loss={avg_loss:8.3f}, "
                              f"ETA={eta/60:.1f}min")
                
                # Calculate final metrics
                training_time = time.time() - training_start
                
                # Performance analysis
                initial_performance = np.mean(episode_rewards[:50]) if len(episode_rewards) >= 50 else np.mean(episode_rewards[:10])
                final_performance = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards[-10:])
                improvement = final_performance - initial_performance
                
                # Stability metrics
                reward_std = np.std(episode_rewards)
                final_stability = np.std(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.std(episode_rewards)
                
                training_results[agent_name] = {
                    'episodes_completed': len(episode_rewards),
                    'initial_performance': initial_performance,
                    'final_performance': final_performance,
                    'improvement': improvement,
                    'improvement_pct': (improvement / abs(initial_performance)) * 100 if initial_performance != 0 else 0,
                    'reward_std': reward_std,
                    'final_stability': final_stability,
                    'total_training_time': training_time,
                    'avg_episode_time': training_time / episodes,
                    'total_reward': sum(episode_rewards),
                    'best_episode': max(episode_rewards),
                    'worst_episode': min(episode_rewards),
                    'success_rate': sum(1 for r in episode_rewards if r > 0) / len(episode_rewards),
                    'converged': improvement > 0
                }
                
                print(f"    ‚úÖ {agent_name} training completed")
                print(f"       Performance: {initial_performance:.3f} ‚Üí {final_performance:.3f}")
                print(f"       Improvement: {improvement:+.3f} ({training_results[agent_name]['improvement_pct']:+.1f}%)")
                print(f"       Training time: {training_time/60:.1f} minutes")
                print(f"       Success rate: {training_results[agent_name]['success_rate']:.1%}")
                
            except Exception as e:
                print(f"    ‚ùå {agent_name} training failed: {e}")
                training_results[agent_name] = {'failed': True, 'error': str(e)}
        
        self.results['training'] = training_results
        return training_results
    
    def comprehensive_evaluation(self, ensemble, env, episodes: int = 100):
        """Comprehensive ensemble evaluation."""
        print(f"\nüìä Comprehensive Ensemble Evaluation ({episodes} episodes)")
        print("-" * 60)
        
        if ensemble is None:
            print("  ‚ö†Ô∏è No ensemble available for evaluation")
            return {}
        
        try:
            episode_rewards = []
            episode_actions = []
            episode_lengths = []
            
            evaluation_start = time.time()
            
            for episode in range(episodes):
                state = env.reset()
                episode_reward = 0
                episode_action_counts = [0, 0, 0]  # Buy, Hold, Sell
                step = 0
                max_steps = 200
                
                while step < max_steps:
                    # State preprocessing
                    if not isinstance(state, torch.Tensor):
                        state = torch.tensor(state, dtype=torch.float32, device=self.device)
                    if state.dim() == 1:
                        state = state.unsqueeze(0)
                    
                    # Ensemble action selection
                    action = ensemble.select_action(state.squeeze(0), deterministic=True)
                    episode_action_counts[action] += 1
                    
                    # Environment step
                    next_state, reward, done, info = env.step(action)
                    episode_reward += reward
                    
                    state = next_state
                    step += 1
                    
                    if done:
                        break
                
                episode_rewards.append(episode_reward)
                episode_actions.append(episode_action_counts)
                episode_lengths.append(step)
                
                if (episode + 1) % 20 == 0:
                    avg_reward = np.mean(episode_rewards[-20:])
                    avg_length = np.mean(episode_lengths[-20:])
                    print(f"    Episode {episode + 1:3d}/{episodes}: "
                          f"Reward={avg_reward:8.3f}, Length={avg_length:.1f}")
            
            evaluation_time = time.time() - evaluation_start
            
            # Comprehensive metrics
            eval_results = {
                'episodes_evaluated': len(episode_rewards),
                'mean_reward': np.mean(episode_rewards),
                'std_reward': np.std(episode_rewards),
                'max_reward': np.max(episode_rewards),
                'min_reward': np.min(episode_rewards),
                'median_reward': np.median(episode_rewards),
                'q75_reward': np.percentile(episode_rewards, 75),
                'q25_reward': np.percentile(episode_rewards, 25),
                'success_rate': sum(1 for r in episode_rewards if r > 0) / len(episode_rewards),
                'total_return': sum(episode_rewards),
                'sharpe_ratio': np.mean(episode_rewards) / np.std(episode_rewards) if np.std(episode_rewards) > 0 else 0,
                'avg_episode_length': np.mean(episode_lengths),
                'evaluation_time': evaluation_time,
                'action_distribution': {
                    'buy_pct': np.mean([actions[0] for actions in episode_actions]) / np.mean(episode_lengths),
                    'hold_pct': np.mean([actions[1] for actions in episode_actions]) / np.mean(episode_lengths),
                    'sell_pct': np.mean([actions[2] for actions in episode_actions]) / np.mean(episode_lengths)
                }
            }
            
            print(f"\n  üìà Comprehensive Evaluation Results:")
            print(f"    Mean reward: {eval_results['mean_reward']:.3f} ¬± {eval_results['std_reward']:.3f}")
            print(f"    Median reward: {eval_results['median_reward']:.3f}")
            print(f"    Best episode: {eval_results['max_reward']:.3f}")
            print(f"    Success rate: {eval_results['success_rate']:.1%}")
            print(f"    Sharpe ratio: {eval_results['sharpe_ratio']:.3f}")
            print(f"    Total return: {eval_results['total_return']:.3f}")
            print(f"    Action distribution: Buy {eval_results['action_distribution']['buy_pct']:.1%}, "
                  f"Hold {eval_results['action_distribution']['hold_pct']:.1%}, "
                  f"Sell {eval_results['action_distribution']['sell_pct']:.1%}")
            
            self.results['evaluation'] = eval_results
            return eval_results
            
        except Exception as e:
            print(f"  ‚ùå Evaluation failed: {e}")
            return {'failed': True, 'error': str(e)}
    
    def save_production_models(self, agents: Dict[str, Any], ensemble):
        """Save trained production models."""
        print("\nüíæ Saving Production Models")
        print("-" * 60)
        
        models_dir = self.output_dir / "production_models"
        models_dir.mkdir(exist_ok=True)
        
        saved_models = {}
        
        # Save individual agents
        for agent_name, agent in agents.items():
            try:
                agent_dir = models_dir / agent_name
                agent_dir.mkdir(exist_ok=True)
                
                # Save agent state
                model_path = agent_dir / "model.pth"
                torch.save({
                    'online_network': agent.online_network.state_dict(),
                    'target_network': agent.target_network.state_dict(),
                    'optimizer': agent.optimizer.state_dict(),
                    'config': agent.config.to_dict(),
                    'training_completed': True
                }, model_path)
                
                saved_models[agent_name] = str(model_path)
                print(f"  ‚úÖ {agent_name} saved to {model_path}")
                
            except Exception as e:
                print(f"  ‚ùå Failed to save {agent_name}: {e}")
        
        # Save ensemble configuration
        if ensemble:
            try:
                ensemble_path = models_dir / "ensemble_config.json"
                ensemble_config = {
                    'strategy': str(ensemble.strategy),
                    'agents': list(agents.keys()),
                    'weights': getattr(ensemble, 'weights', None),
                    'confidence_threshold': getattr(ensemble, 'confidence_threshold', None),
                    'created_at': self.timestamp
                }
                
                with open(ensemble_path, 'w') as f:
                    json.dump(ensemble_config, f, indent=2)
                
                saved_models['ensemble'] = str(ensemble_path)
                print(f"  ‚úÖ Ensemble config saved to {ensemble_path}")
                
            except Exception as e:
                print(f"  ‚ùå Failed to save ensemble config: {e}")
        
        self.results['saved_models'] = saved_models
        print(f"  üìÅ Models saved to: {models_dir}")
        
        return saved_models
    
    def save_complete_results(self):
        """Save comprehensive training results."""
        print("\nüíæ Saving Complete Results")
        print("-" * 60)
        
        # Finalize results
        self.results['end_time'] = time.time()
        self.results['total_duration'] = self.results['end_time'] - self.results['start_time']
        
        # Save detailed JSON results
        results_file = self.output_dir / f"complete_production_results_{self.timestamp}.json"
        
        def json_serialize(obj):
            if isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj
        
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=json_serialize)
        
        # Create summary report
        summary_file = self.output_dir / f"complete_training_summary_{self.timestamp}.md"
        
        with open(summary_file, 'w') as f:
            f.write(f"# Complete Production Training Results\n\n")
            f.write(f"**Session:** {self.timestamp}  \n")
            f.write(f"**Duration:** {self.results['total_duration']/3600:.2f} hours  \n")
            f.write(f"**Device:** {self.results['device']}  \n\n")
            
            # Environment summary
            if 'environment' in self.results:
                env = self.results['environment']
                f.write(f"## Environment\n")
                f.write(f"- **State Dimension:** {env['state_dim']}\n")
                f.write(f"- **Action Dimension:** {env['action_dim']}\n")
                f.write(f"- **Dataset Size:** {env['dataset_size']:,} timesteps\n")
                f.write(f"- **Feature Type:** {env['feature_type']}\n\n")
            
            # Training results
            if 'training' in self.results:
                f.write(f"## Training Results\n")
                for agent_name, results in self.results['training'].items():
                    if not results.get('failed'):
                        f.write(f"### {agent_name}\n")
                        f.write(f"- **Episodes:** {results['episodes_completed']}\n")
                        f.write(f"- **Initial Performance:** {results['initial_performance']:.3f}\n")
                        f.write(f"- **Final Performance:** {results['final_performance']:.3f}\n")
                        f.write(f"- **Improvement:** {results['improvement']:+.3f} ({results['improvement_pct']:+.1f}%)\n")
                        f.write(f"- **Success Rate:** {results['success_rate']:.1%}\n")
                        f.write(f"- **Training Time:** {results['total_training_time']/60:.1f} minutes\n\n")
            
            # Evaluation results
            if 'evaluation' in self.results and not self.results['evaluation'].get('failed'):
                eval_res = self.results['evaluation']
                f.write(f"## Ensemble Evaluation\n")
                f.write(f"- **Mean Reward:** {eval_res['mean_reward']:.3f} ¬± {eval_res['std_reward']:.3f}\n")
                f.write(f"- **Sharpe Ratio:** {eval_res['sharpe_ratio']:.3f}\n")
                f.write(f"- **Success Rate:** {eval_res['success_rate']:.1%}\n")
                f.write(f"- **Total Return:** {eval_res['total_return']:.3f}\n")
        
        print(f"  ‚úÖ Results saved to: {results_file}")
        print(f"  ‚úÖ Summary saved to: {summary_file}")
        print(f"  ‚è±Ô∏è Total duration: {self.results['total_duration']/3600:.2f} hours")
        
        return results_file, summary_file

def main():
    """Main complete training execution."""
    print("üöÄ FinRL Contest 2024 - Complete Production Training")
    print("=" * 80)
    print("‚ö†Ô∏è  WARNING: This is full-scale training that may take several hours!")
    print("=" * 80)
    
    try:
        # Initialize trainer
        trainer = CompleteProductionTrainer()
        
        # Setup environment
        env, state_dim, action_dim, data = trainer.setup_trading_environment()
        
        # Create production agents
        agents = trainer.create_production_agents(state_dim, action_dim)
        
        if not agents:
            print("‚ùå No agents created - cannot proceed with training")
            return
        
        # Create ensemble
        ensemble = trainer.create_production_ensemble(agents)
        
        # Complete training (100 episodes for demonstration - change to 500+ for full training)
        print(f"\n‚ö†Ô∏è  Starting complete training - running 100 episodes for validation...")
        training_results = trainer.complete_agent_training(agents, env, episodes=100)
        
        # Mark training progress
        print(f"\n‚úÖ Agent training completed! Starting ensemble evaluation...")
        
        # Comprehensive evaluation
        if ensemble:
            eval_results = trainer.comprehensive_evaluation(ensemble, env, episodes=50)
        
        # Save models
        model_paths = trainer.save_production_models(agents, ensemble)
        
        # Save results
        results_file, summary_file = trainer.save_complete_results()
        
        print("\n" + "=" * 80)
        print("üèÜ COMPLETE PRODUCTION TRAINING FINISHED!")
        print("=" * 80)
        
        # Final summary
        if 'training' in trainer.results:
            print("\nüìä Final Training Summary:")
            for agent_name, results in trainer.results['training'].items():
                if not results.get('failed'):
                    print(f"  {agent_name}: {results['improvement']:+.3f} improvement "
                          f"({results['improvement_pct']:+.1f}%) over {results['episodes_completed']} episodes")
        
        if 'evaluation' in trainer.results and not trainer.results['evaluation'].get('failed'):
            eval_info = trainer.results['evaluation']
            print(f"\nüéØ Final Ensemble Performance:")
            print(f"  Mean Reward: {eval_info['mean_reward']:.3f} ¬± {eval_info['std_reward']:.3f}")
            print(f"  Sharpe Ratio: {eval_info['sharpe_ratio']:.3f}")
            print(f"  Success Rate: {eval_info['success_rate']:.1%}")
        
        print(f"\nüìÅ Results and models saved to: {trainer.output_dir}")
        print(f"‚è±Ô∏è Total training time: {trainer.results['total_duration']/3600:.2f} hours")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\n\nüõë Training interrupted by user")
    except Exception as e:
        print(f"\n\nüí• Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()