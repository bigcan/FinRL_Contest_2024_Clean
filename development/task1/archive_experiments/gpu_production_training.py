#!/usr/bin/env python3
"""
GPU-Enabled Production Training - FORCES GPU usage for training
Addresses the critical issue where training was running on CPU instead of GPU
"""

import os
import sys
import time
import json
import torch
from pathlib import Path
from datetime import datetime

# Add src path
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir / "src"))

def verify_gpu_setup():
    """Verify GPU is available and configured correctly."""
    print("ğŸ” GPU Configuration Check")
    print("-" * 40)
    
    # Check CUDA availability
    if not torch.cuda.is_available():
        print("âŒ CUDA not available! Training will fail.")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"âœ… CUDA available: {torch.cuda.is_available()}")
    print(f"âœ… GPU devices: {gpu_count}")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        print(f"âœ… GPU {i}: {gpu_name}")
    
    # Test GPU memory allocation
    try:
        test_tensor = torch.randn(1000, 1000).cuda()
        print(f"âœ… GPU memory test passed")
        del test_tensor
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"âŒ GPU memory test failed: {e}")
        return False
    
    return True

def run_gpu_training():
    """Run production training with ENFORCED GPU usage."""
    
    print("ğŸš€ GPU-ENFORCED Production Training")
    print("=" * 60)
    
    # CRITICAL: Verify GPU setup before starting
    if not verify_gpu_setup():
        print("ğŸ’¥ GPU setup failed. Cannot continue.")
        return False
    
    try:
        # Import training components
        from task1_ensemble import run
        from erl_agent import AgentD3QN, AgentDoubleDQN, AgentPrioritizedDQN
        
        print("ğŸ¯ Training Configuration:")
        print(f"   ğŸ”¥ GPU: ENFORCED (device 0)")
        print(f"   ğŸ“Š Data: Enhanced v3 Bitcoin LOB (823K timesteps)")
        print(f"   ğŸ¯ Agents: D3QN, DoubleDQN, PrioritizedDQN")
        print(f"   ğŸš€ Features: 41-dimensional enhanced microstructure")
        print("")
        
        # Set CUDA device explicitly
        torch.cuda.set_device(0)
        print(f"âœ… CUDA device set to: {torch.cuda.current_device()}")
        
        # Force GPU usage environment variables
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_path = f"gpu_training_{timestamp}"
        
        print(f"ğŸ“ Save path: {save_path}")
        print(f"â³ Starting GPU training...")
        print("")
        
        # Run training with GPU enforcement using correct function signature
        agent_list = [AgentD3QN, AgentDoubleDQN, AgentPrioritizedDQN]  # Use actual classes
        config_dict = {
            'gpu_id': 0,  # EXPLICIT GPU ID = 0
            'starting_cash': 100000,
            'env_class': 'TradeSimulator'
        }
        
        result = run(
            save_path=save_path,
            agent_list=agent_list,
            log_rules=["print_time", "save_model"],
            config_dict=config_dict
        )
        
        print("")
        print("âœ… GPU Production Training Completed!")
        print("=" * 60)
        
        return result
        
    except Exception as e:
        print(f"ğŸ’¥ GPU training failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def monitor_gpu_usage():
    """Monitor GPU usage during training."""
    import subprocess
    import threading
    import time
    
    def gpu_monitor():
        while True:
            try:
                result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    gpu_util, mem_used, mem_total = result.stdout.strip().split(', ')
                    print(f"ğŸ”¥ GPU: {gpu_util}% | Memory: {mem_used}/{mem_total} MB")
            except Exception as e:
                print(f"âš ï¸ GPU monitor error: {e}")
                break
            time.sleep(30)  # Check every 30 seconds
    
    monitor_thread = threading.Thread(target=gpu_monitor, daemon=True)
    monitor_thread.start()
    print("ğŸ“Š GPU monitoring started (every 30 seconds)")

def main():
    """Main GPU training function with monitoring."""
    
    print("ğŸš€ FinRL Contest 2024 - GPU-ENFORCED Production Training")
    print("=" * 70)
    print("ğŸ”¥ CRITICAL FIX: Forces GPU usage (previous runs used CPU)")
    print("âš¡ Expected 10-20x speed improvement with GPU acceleration")
    print("ğŸ¯ Goal: Competition-ready models with >60% accuracy")
    print("=" * 70)
    
    # Start GPU monitoring
    monitor_gpu_usage()
    
    start_time = time.time()
    
    try:
        # Run GPU training
        success = run_gpu_training()
        
        elapsed_hours = (time.time() - start_time) / 3600
        
        if success:
            print("\\n" + "=" * 70)
            print("ğŸ‰ GPU PRODUCTION TRAINING COMPLETED SUCCESSFULLY!")
            print("=" * 70)
            print(f"â±ï¸  Duration: {elapsed_hours:.2f} hours")
            print(f"ğŸ”¥ GPU acceleration enabled throughout training")
            print(f"ğŸ“Š Models ready for competition evaluation")
            print("=" * 70)
            return 0
        else:
            print("\\nğŸ’¥ GPU training failed")
            return 1
        
    except Exception as e:
        print(f"\\nğŸ’¥ GPU training crashed: {e}")
        return 1

if __name__ == "__main__":
    exit(main())